{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from clear_nbcode import *\n",
    "# clear_nbcode('02b_initializing-Copy1.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(x): return x.mean(), x.std()\n",
    "\n",
    "def pp(*args, n=75):\n",
    "    for arg in args:\n",
    "        print(arg)\n",
    "        print(\"-\"*n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Why you need a good init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand why initialization is important in a neural net, we'll focus on the basic operation you have there: matrix multiplications. So let's just take a vector `x`, and a matrix `a` initiliazed randomly, then multiply them 100 times (as if we had 100 layers). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_when initialization of weights is too high_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0.0018), tensor(0.9480))\n",
      "---------------------------------------------------------------------------\n",
      "(tensor(-0.0035), tensor(1.0018))\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "a = torch.randn(512, 512)\n",
    "pp(stats(x), stats(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor(nan), tensor(nan)), 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    x = a @ x\n",
    "    if torch.isnan(x.std()): break\n",
    "stats(x), i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_when initialization of weights is too low_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(-0.0280), tensor(0.9478))\n",
      "---------------------------------------------------------------------------\n",
      "(tensor(-1.6596e-05), tensor(0.0100))\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "a = torch.randn(512, 512) * 0.01\n",
    "pp(stats(x), stats(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(0.))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    x = a@x\n",
    "stats(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.006866583323851228, 22.626899718929742)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var = 0.,0.\n",
    "for i in range(10000):\n",
    "    x = torch.randn(512)\n",
    "    a = torch.randn(512, 512)\n",
    "    y = a@x\n",
    "    mean += y.mean().item()\n",
    "    var += y.pow(2).mean().item()\n",
    "mean/10000, math.sqrt(var/10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.627416997969522"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0045433636779020166, 1.001365381042477)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var = 0.,0.\n",
    "for i in range(10000):\n",
    "    x = torch.randn(1)\n",
    "    a = torch.randn(1)\n",
    "    y = a*x\n",
    "    mean += y.mean().item()\n",
    "    var += y.pow(2).mean().item()\n",
    "mean/10000, math.sqrt(var/10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0006665773250747219, 0.044218858381667475)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var = 0.,0.\n",
    "for i in range(10000):\n",
    "    x = torch.randn(1)\n",
    "    a = torch.randn(1)*math.sqrt(1/512.)\n",
    "    y = a*x\n",
    "    mean += y.mean().item()\n",
    "    var += y.pow(2).mean().item()\n",
    "mean/10000, math.sqrt(var/10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001953125"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0639), tensor(1.0590))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var = 0.,0.\n",
    "for i in range(10000):\n",
    "    x = torch.randn(512)\n",
    "    a = torch.randn(512, 512)*math.sqrt(1/512.)\n",
    "    x = a@x\n",
    "\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### using activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x): return torch.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0073), tensor(0.1213))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "a = torch.randn(512, 512)*math.sqrt(1/512.)\n",
    "for i in range(100):\n",
    "    x = tanh(a@x)\n",
    "\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0001461052545811981, 0.6276535678475743)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var = 0.,0.\n",
    "for i in range(10000):\n",
    "    x = torch.randn(512)\n",
    "    a = torch.randn(512, 512)*math.sqrt(1/512.)\n",
    "    y = tanh(a@x)\n",
    "    mean += y.mean().item()\n",
    "    var += y.pow(2).mean().item()\n",
    "mean/10000, math.sqrt(var/10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the std is good, but not 1. To address this issue:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before Xavier Glorot and Yoshua Bengio published their landmark paper titled _Understanding the difficulty of training deep feedforward neural networks_, the “commonly used heuristic” to which they compared their experiments was that of _initializing weights from a uniform distribution in [-1,1]_ and then scaling by **1/√n.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(9.0958e-26), tensor(3.1799e-24))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "a = torch.Tensor(512, 512).uniform_(-1, 1) * math.sqrt(1/512.)\n",
    "for i in range(100):\n",
    "    x = tanh(a@x)\n",
    "\n",
    "stats(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(0.))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "a = torch.Tensor(512, 512).uniform_(-1, 1) * math.sqrt(1/512.)\n",
    "for i in range(1000):\n",
    "    x = tanh(a@x)\n",
    "\n",
    "stats(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### _**Xavier Init:**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xavier initialization sets a layer’s weights to values chosen from a random uniform distribution that’s bounded between:\n",
    "$$\\Biggl[\n",
    "-\\frac{\\sqrt{6}}{\\sqrt{n_{i}+n_{i+1}}} , + \\frac{\\sqrt{6}}{\\sqrt{n_{i}+n_{i+1}}}\n",
    "\\Biggr]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here, $n_{i}$ is `fan-in` i.e. `number of incoming network connections to that layer`$,\\&$ <br>  $n_{i+1}$ is `fan-out` i.e. `number of outgoing network connections from that layer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier(m,h): return torch.Tensor(512, 512).uniform_(-1, 1) * math.sqrt(6./(m+h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0100), tensor(0.1266))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tanh activation\n",
    "x = torch.randn(512)\n",
    "a = xavier(512, 512)\n",
    "for i in range(10000):\n",
    "    x = tanh(a@x)\n",
    "\n",
    "stats(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  when using relu activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): return x.clamp_min(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor(0.), tensor(0.)), 260)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relu activation with [xavier init]\n",
    "x = torch.randn(512)\n",
    "a = xavier(512, 512)\n",
    "\n",
    "for i in range(10000):\n",
    "    x = relu(a@x)\n",
    "    if x.std() == 0.: break \n",
    "\n",
    "stats(x), i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(6.5335e-16), tensor(8.8455e-16))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relu activation with [random init scaled by 1/sqrt(n)]\n",
    "x = torch.randn(512)\n",
    "a = torch.randn(512, 512)*math.sqrt(1/512.)\n",
    "for i in range(100):\n",
    "    x = relu(a@x)\n",
    "\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.25283432006836, 16.087787123506143)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relu activation with [random init]\n",
    "mean, var = 0.,0.\n",
    "x = torch.randn(512)\n",
    "a = torch.randn(512, 512)\n",
    "for i in range(10000):\n",
    "    y = relu(a@x)\n",
    "    mean += y.mean().item()\n",
    "    var += y.pow(2).mean().item()\n",
    "mean/10000, math.sqrt(var/10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22.627416997969522, 16.0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt(512), math.sqrt(512/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5680713653564453, 1.0546854089786852)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relu activation with [random init scaled by sqrt(2/n)]\n",
    "mean, var = 0.,0.\n",
    "x = torch.randn(512)\n",
    "a = torch.randn(512, 512)*math.sqrt(2./512)\n",
    "for i in range(10000):\n",
    "    y = relu(a@x)\n",
    "    mean += y.mean().item()\n",
    "    var += y.pow(2).mean().item()\n",
    "mean/10000, math.sqrt(var/10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keeping the standard deviation of layers’ activations around 1 will allow us to stack several more layers in a deep neural network without gradients exploding or vanishing <br>\n",
    "here mean is ~= .5 because in relu we are removing nearly half of the values (with zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **Kaiming Init:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. _Initialize weights with **random numbers from standard normal distribution** (with appropiate weight dim at the layer)_ <br>\n",
    "2. _Multiply each randomly chosen number by **√2/√n** where **n is the number of incoming connections** coming into a given layer from the previous layer’s output (also known as the **“fan-in”**)._\n",
    "3. _**Bias** tensors are initialized to **zero**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "std: $$\\sigma=\\sqrt{\\frac{2}{n_{l}+\\hat{n}_{l}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here, $n_{l}$ is `fan-in` i.e. `number of incoming network connections to that layer`$,\\&$ <br>  $\\hat{n}_{l}$ is `fan-out` i.e. `number of outgoing network connections from that layer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming(m,h): return torch.randn(m,h)*math.sqrt(2./m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4102), tensor(0.6081))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "for i in range(100):\n",
    "    a = kaiming(512, 512)\n",
    "    x = relu(a@x)\n",
    "\n",
    "stats(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1363), tensor(0.1861))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "for i in range(1000):\n",
    "    a = kaiming(512, 512)\n",
    "    x = relu(a@x)\n",
    "\n",
    "stats(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor(3.2230e-44), tensor(4.6243e-44)), 99999)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "for i in range(100000):\n",
    "    a = kaiming(512, 512)\n",
    "    x = relu(a@x)\n",
    "    if x.std() == 0. : break\n",
    "\n",
    "stats(x),i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "don't know it must have worked, but for 100k layers its weight is too close to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
